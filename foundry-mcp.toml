# foundry-mcp Configuration
#
# Provider priority with model selection for AI consultation workflows.

[workspace]
specs_dir = "./specs"
# bikelane_dir = "./specs/.bikelane"  # Optional: custom path for intake queue storage

[logging]
level = "INFO"
structured = true

[server]
name = "foundry-mcp"
version = "0.5.0"
disabled_tools = ["error", "health", "metrics", "test"]

[workflow]
mode = "autonomous"
auto_validate = true
journal_enabled = true

[test]
default_runner = "pytest"

# Provider availability detection cache (seconds)
[providers]
availability_cache_ttl = 3600

[consultation]
# Provider priority list - first available provider wins
# Format: "[api]provider/model" or "[cli]transport[:backend/model|:model]"
priority = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.1-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]claude:opus",
]

# Operational settings
# Per-provider timeout minimum: 360s for AI CLI providers
default_timeout = 360
max_retries = 2
retry_delay = 5.0
fallback_enabled = true
cache_ttl = 3600

# Per-workflow consultation configuration
# Each workflow can specify min_models for consensus and timeout override

# Fidelity review requires 2 models for consensus to catch implementation deviations
[consultation.workflows.fidelity_review]
min_models = 2
timeout_override = 600.0

# Plan review requires 2 models for consensus on architectural decisions
[consultation.workflows.plan_review]
min_models = 2
timeout_override = 600.0  # Whole workflow timeout

# Markdown plan review (pre-spec) requires 2 models for consensus
[consultation.workflows.markdown_plan_review]
min_models = 2
timeout_override = 600.0  # Whole workflow timeout

# Doc generation uses single model (default min_models=1)
[consultation.workflows.doc_generation]
min_models = 1

[implement]
# Default flags for /implement command (can be overridden via CLI flags)
auto = false      # --auto: skip prompts between tasks
delegate = true   # --delegate: use subagent(s) for implementation
parallel = false  # --parallel: run subagents concurrently (implies delegate)
model = "haiku"   # --model: model for delegated tasks (haiku, sonnet, opus)

[git]
enabled = true
commit_cadence = "task"

# Error collection - stores error logs for observability and debugging
[error_collection]
enabled = true
# storage_path defaults to ~/.foundry-mcp/errors
retention_days = 30
max_records = 10000

# Metrics persistence - stores metrics for historical analysis
[metrics_persistence]
enabled = true
# storage_path defaults to ~/.foundry-mcp/metrics
retention_days = 7
max_records = 50000

# Research workflow configuration
[research]
enabled = true
# Default provider - supports ProviderSpec format: "[cli]gemini:pro" or simple: "gemini"
default_provider = "[cli]gemini:pro"
ttl_hours = 24
max_messages_per_thread = 100

# Deep research settings
deep_research_max_iterations = 3
deep_research_max_sub_queries = 5
deep_research_max_sources = 5
deep_research_follow_links = true
deep_research_timeout = 600.0  # Whole workflow timeout
deep_research_max_concurrent = 3
deep_research_audit_artifacts = true
# Research mode: "general" | "academic" | "technical"
deep_research_mode = "technical"
deep_research_providers = ["tavily", "perplexity", "google", "semantic_scholar"]

# Per-phase timeouts (seconds) - minimum 360s for AI CLI providers
deep_research_planning_timeout = 360.0
deep_research_analysis_timeout = 360.0
deep_research_synthesis_timeout = 600.0  # Synthesis may take longer
deep_research_refinement_timeout = 360.0

# Per-phase providers (supports ProviderSpec format)
# Use Claude for synthesis (report writing) for better prose
deep_research_planning_provider = "[cli]gemini:gemini-3-flash-preview"
deep_research_analysis_provider = "[cli]gemini:gemini-3-flash-preview"
deep_research_synthesis_provider = "[cli]gemini:gemini-3-flash-preview"
deep_research_refinement_provider = "[cli]gemini:gemini-3-flash-preview"

# Search rate limiting
search_rate_limit = 60
max_concurrent_searches = 3

[research.per_provider_rate_limits]
tavily = 60
perplexity = 60
google = 100
semantic_scholar = 100

#!/usr/bin/env python3
"""
Core library for querying machine-readable codebase documentation.

This library provides functions to load and query codebase.json files
generated by the `sdd doc generate` command.
"""

import json
import re
import textwrap
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Iterable
from dataclasses import dataclass


@dataclass
class QueryResult:
    """Represents a query result with metadata."""
    entity_type: str  # 'class', 'function', 'module', 'dependency'
    name: str
    data: Dict[str, Any]
    relevance_score: float = 1.0  # For ranking results


class DocumentationQuery:
    """Main class for querying codebase documentation."""

    def __init__(self, docs_path: Optional[str] = None):
        """
        Initialize the query engine.

        Args:
            docs_path: Path to the codebase.json file or its directory.
                      If None, will search for docs/ in current directory.
                      Prefers codebase.json if both exist.
        """
        self.docs_path = self._resolve_docs_path(docs_path)
        self.data: Optional[Dict] = None
        self._loaded = False
        self._module_cache: Dict[str, Dict[str, Any]] = {}
        self._classes_by_module: Dict[str, List[Dict[str, Any]]] = {}
        self._functions_by_module: Dict[str, List[Dict[str, Any]]] = {}
        self._reverse_dependencies: Dict[str, List[str]] = {}

    def _resolve_docs_path(self, docs_path: Optional[str]) -> Path:
        """Resolve the documentation path with auto-detection."""
        if docs_path is None:
            # Auto-detect: Search multiple common locations
            docs_path = self._find_documentation_dir()
        else:
            docs_path = Path(docs_path)

        # If it's a directory, look for codebase.json
        if docs_path.is_dir():
            docs_path = docs_path / "codebase.json"

        return docs_path

    def _find_documentation_dir(self) -> Path:
        """
        Auto-detect documentation directory by searching common locations.

        Returns:
            Path to documentation directory (may not exist)
        """
        search_locations = [
            Path.cwd() / "docs",                    # Current directory
            Path.cwd().parent / "docs",              # Parent directory
            Path.cwd() / "documentation",            # Alternative name
            Path.home() / ".claude" / "docs",        # Claude home
        ]

        # Search for existing codebase.json
        for location in search_locations:
            codebase_json = location / "codebase.json"
            if codebase_json.exists():
                return location

        # If none found, return default (current dir / docs)
        return Path.cwd() / "docs"

    def load(self) -> bool:
        """
        Load the documentation file.

        Returns:
            True if successful, False otherwise
        """
        try:
            if not self.docs_path.exists():
                return False

            with open(self.docs_path, 'r') as f:
                raw_data = json.load(f)
                self.data = self._normalize_data(raw_data)
                self._reindex()

            self._loaded = True
            return True
        except Exception as e:
            print(f"Error loading documentation: {e}")
            return False

    def _normalize_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize documentation payload to expected schema."""
        if not isinstance(raw_data, dict):
            raise ValueError("documentation payload must be a dictionary")

        data: Dict[str, Any] = dict(raw_data)

        classes = []
        for cls in raw_data.get('classes', []) or []:
            if not isinstance(cls, dict):
                continue
            normalized = dict(cls)
            normalized.setdefault('name', '')
            normalized.setdefault('file', '')
            normalized.setdefault('line', None)
            normalized.setdefault('docstring', '')
            normalized.setdefault('bases', [])
            normalized.setdefault('methods', [])
            normalized.setdefault('properties', [])
            classes.append(normalized)
        data['classes'] = classes

        functions = []
        for func in raw_data.get('functions', []) or []:
            if not isinstance(func, dict):
                continue
            normalized = dict(func)
            normalized.setdefault('name', '')
            normalized.setdefault('file', '')
            normalized.setdefault('line', None)
            normalized.setdefault('docstring', '')
            normalized.setdefault('parameters', [])
            normalized.setdefault('return_type', None)
            normalized.setdefault('decorators', [])
            normalized.setdefault('complexity', 0)
            normalized.setdefault('is_async', False)
            # Schema v2.0: Cross-reference fields
            normalized.setdefault('callers', [])
            normalized.setdefault('calls', [])
            normalized.setdefault('call_count', None)
            functions.append(normalized)
        data['functions'] = functions

        dependencies: Dict[str, List[str]] = {}
        raw_dependencies = raw_data.get('dependencies', {}) or {}
        if isinstance(raw_dependencies, dict):
            for key, value in raw_dependencies.items():
                if not isinstance(value, Iterable) or isinstance(value, (str, bytes)):
                    dependencies[str(key)] = []
                else:
                    dependencies[str(key)] = [str(item) for item in value]
        data['dependencies'] = dependencies

        metadata = raw_data.get('metadata') or {}
        if not isinstance(metadata, dict):
            metadata = {}

        generated_at = metadata.get('generated_at') or raw_data.get('generated_at') or 'unknown'
        metadata.setdefault('generated_at', generated_at)
        metadata.setdefault('project_name', raw_data.get('project_name', 'unknown'))
        metadata.setdefault('version', raw_data.get('version', 'unknown'))

        if 'language' in raw_data and 'language' not in metadata:
            metadata['language'] = raw_data['language']

        if 'languages' in metadata:
            languages = metadata['languages']
            if isinstance(languages, str):
                metadata['languages'] = [languages]
            elif isinstance(languages, Iterable):
                metadata['languages'] = [str(lang) for lang in languages if lang]
            else:
                metadata['languages'] = []
        else:
            language = metadata.get('language')
            if language:
                metadata['languages'] = [str(language)]
            else:
                metadata['languages'] = [str(lang) for lang in raw_data.get('languages', []) if lang]

        data['metadata'] = metadata
        data['generated_at'] = metadata.get('generated_at', 'unknown')

        modules_raw = raw_data.get('modules', []) or []
        modules: List[Dict[str, Any]] = []
        module_files_seen = set()
        if isinstance(modules_raw, list):
            for module in modules_raw:
                if not isinstance(module, dict):
                    continue
                normalized = dict(module)
                file_path = normalized.get('file') or normalized.get('name') or ''
                normalized['file'] = str(file_path)
                normalized['name'] = normalized.get('name') or Path(str(file_path)).stem
                normalized['docstring'] = normalized.get('docstring', '') or ''
                normalized['imports'] = list(normalized.get('imports', []) or [])
                normalized['classes'] = list(normalized.get('classes', []) or [])
                normalized['functions'] = list(normalized.get('functions', []) or [])
                normalized['lines'] = normalized.get('lines')
                modules.append(normalized)
                if normalized['file']:
                    module_files_seen.add(normalized['file'])

        entity_module_files = {
            cls.get('file') for cls in classes if cls.get('file')
        }.union({
            func.get('file') for func in functions if func.get('file')
        })

        for module_file in sorted(entity_module_files):
            if module_file not in module_files_seen:
                modules.append({
                    'name': Path(module_file).stem,
                    'file': module_file,
                    'docstring': '',
                    'classes': [cls['name'] for cls in classes if cls.get('file') == module_file],
                    'functions': [func['name'] for func in functions if func.get('file') == module_file],
                    'imports': [],
                    'lines': None
                })

        modules.sort(key=lambda m: m.get('file') or m.get('name') or '')
        data['modules'] = modules

        statistics = raw_data.get('statistics') or {}
        if not isinstance(statistics, dict):
            statistics = {}

        complexities = [func.get('complexity', 0) for func in functions if isinstance(func, dict)]
        total_modules = len({module.get('file') for module in modules if module.get('file')})
        statistics.setdefault('total_classes', len(classes))
        statistics.setdefault('total_functions', len(functions))
        statistics.setdefault('total_modules', total_modules)
        statistics.setdefault('total_files', statistics.get('total_modules', total_modules))
        statistics.setdefault('total_lines', statistics.get('total_lines', 0))

        avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        max_complexity = max(complexities) if complexities else 0
        high_complexity = [func.get('name') for func in functions if func.get('complexity', 0) >= 5]

        statistics.setdefault('avg_complexity', round(avg_complexity, 2))
        statistics.setdefault('max_complexity', max_complexity)
        statistics.setdefault('high_complexity_count', len(high_complexity))
        statistics.setdefault('high_complexity_functions', high_complexity)

        data['statistics'] = statistics

        return data

    def _reindex(self) -> None:
        """Build in-memory indexes for fast lookups."""
        self._module_cache = {}
        self._classes_by_module = {}
        self._functions_by_module = {}
        self._reverse_dependencies = {}

        if not self.data:
            return

        for cls in self.data.get('classes', []):
            module = cls.get('file', '')
            self._classes_by_module.setdefault(module, []).append(cls)

        for func in self.data.get('functions', []):
            module = func.get('file', '')
            self._functions_by_module.setdefault(module, []).append(func)

        dependencies = self.data.get('dependencies', {}) or {}
        reverse_map: Dict[str, set] = {}
        for mod, deps in dependencies.items():
            for dep in deps:
                reverse_map.setdefault(dep, set()).add(mod)

        self._reverse_dependencies = {
            key: sorted(values) for key, values in reverse_map.items()
        }

        modules = self.data.get('modules', []) or []
        seen_files = set()
        for module_meta in modules:
            file_path = module_meta.get('file') or module_meta.get('name') or ''
            if not file_path:
                continue
            seen_files.add(file_path)
            self._module_cache[file_path] = self._assemble_module_entry(file_path, module_meta)

        # Include modules inferred from entities even if absent from modules list
        entity_modules = set(self._classes_by_module).union(self._functions_by_module)
        for file_path in entity_modules - seen_files:
            self._module_cache[file_path] = self._assemble_module_entry(file_path, None)

    def _assemble_module_entry(self, file_path: str, module_meta: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Compose module information from stored data."""
        module_meta = dict(module_meta) if module_meta else {}
        name = module_meta.get('name') or Path(file_path).stem or file_path
        docstring = module_meta.get('docstring', '') or ''
        imports = list(module_meta.get('imports', []) or [])
        lines = module_meta.get('lines')

        classes = list(self._classes_by_module.get(file_path, []))
        functions = list(self._functions_by_module.get(file_path, []))
        dependencies = list(self.data.get('dependencies', {}).get(file_path, []))
        reverse_dependencies = list(self._reverse_dependencies.get(file_path, []))

        statistics = {
            'class_count': len(classes),
            'function_count': len(functions),
            'dependency_count': len(dependencies),
            'reverse_dependency_count': len(reverse_dependencies),
            'lines': lines,
            **self._compute_module_complexity(functions)
        }

        return {
            'name': name,
            'file': file_path,
            'docstring': docstring,
            'docstring_excerpt': self._get_docstring_excerpt(docstring),
            'imports': imports,
            'lines': lines,
            'classes': classes,
            'functions': functions,
            'dependencies': dependencies,
            'reverse_dependencies': reverse_dependencies,
            'statistics': statistics,
            'class_count': statistics['class_count'],
            'function_count': statistics['function_count'],
            'dependency_count': statistics['dependency_count'],
            'reverse_dependency_count': statistics['reverse_dependency_count'],
            'avg_complexity': statistics['avg_complexity'],
            'max_complexity': statistics['max_complexity'],
            'high_complexity_count': statistics['high_complexity_count']
        }

    @staticmethod
    def _compute_module_complexity(functions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compute complexity-oriented statistics for a module."""
        complexities = [func.get('complexity', 0) for func in functions]
        if not complexities:
            return {
                'avg_complexity': 0,
                'max_complexity': 0,
                'high_complexity_count': 0,
                'high_complexity_functions': []
            }

        avg = sum(complexities) / len(complexities)
        max_val = max(complexities)
        high = [func.get('name') for func in functions if func.get('complexity', 0) >= 5]
        return {
            'avg_complexity': round(avg, 2),
            'max_complexity': max_val,
            'high_complexity_count': len(high),
            'high_complexity_functions': high
        }

    @staticmethod
    def _get_docstring_excerpt(docstring: Optional[str], max_length: int = 160) -> str:
        """Generate a concise excerpt from a docstring."""
        if not docstring:
            return ''

        condensed = ' '.join(line.strip() for line in str(docstring).strip().splitlines() if line.strip())
        if not condensed:
            return ''

        return textwrap.shorten(condensed, width=max_length, placeholder='...')

    def _resolve_module_key(self, module_path: str) -> str:
        """Resolve various aliases to a canonical module key."""
        if module_path in self._module_cache:
            return module_path

        normalized = module_path.lstrip('./')
        for key in self._module_cache:
            if key == normalized:
                return key
            if key.endswith(normalized):
                return key
            name = self._module_cache[key].get('name')
            if name and (name == normalized or name.endswith(normalized)):
                return key
            stem = Path(key).stem
            if stem == normalized or stem == Path(normalized).stem:
                return key

        return module_path

    @staticmethod
    def _copy_module_info(module_info: Dict[str, Any]) -> Dict[str, Any]:
        """Return a safe copy of module metadata for public consumption."""
        return {
            'name': module_info.get('name'),
            'file': module_info.get('file'),
            'docstring': module_info.get('docstring'),
            'docstring_excerpt': module_info.get('docstring_excerpt'),
            'imports': list(module_info.get('imports', [])),
            'lines': module_info.get('lines'),
            'classes': [dict(cls) for cls in module_info.get('classes', [])],
            'functions': [dict(func) for func in module_info.get('functions', [])],
            'dependencies': list(module_info.get('dependencies', [])),
            'reverse_dependencies': list(module_info.get('reverse_dependencies', [])),
            'statistics': dict(module_info.get('statistics', {}))
        }

    @staticmethod
    def apply_pattern_filter(
        items: Iterable[Dict[str, Any]],
        name: str,
        pattern: bool = False,
        key_func: Optional[callable] = None
    ) -> List[Dict[str, Any]]:
        """
        Filter items by name or pattern.

        Provides consistent pattern matching behavior across query methods.
        Supports both exact string matching and regex pattern matching.

        Args:
            items: Iterable of items to filter (typically dicts with 'name' key)
            name: Name or regex pattern to match against
            pattern: If True, treat name as regex pattern (case-insensitive).
                    If False, perform exact string match.
            key_func: Optional function to extract the field to match.
                     Defaults to lambda x: x.get('name', '').
                     Should return a string to match against.

        Returns:
            List of matching items from the input iterable.

        Raises:
            re.error: If pattern=True and name is an invalid regex pattern.

        Examples:
            >>> items = [{'name': 'Calculator'}, {'name': 'Validator'}]
            >>> # Exact match
            >>> results = apply_pattern_filter(items, 'Calculator', pattern=False)
            >>> len(results)
            1

            >>> # Regex pattern (case-insensitive)
            >>> results = apply_pattern_filter(items, 'calc', pattern=True)
            >>> len(results)
            1

            >>> # Custom key function
            >>> items = [{'file': 'src/calc.py'}, {'file': 'src/valid.py'}]
            >>> results = apply_pattern_filter(
            ...     items, 'calc', pattern=True,
            ...     key_func=lambda x: x.get('file', '')
            ... )
            >>> len(results)
            1

        Note:
            - Pattern matching is case-insensitive
            - Items where key_func returns empty string or None are skipped
            - Invalid regex patterns will raise re.error (not caught internally)
        """
        if key_func is None:
            key_func = lambda x: x.get('name', '')

        results = []

        if pattern:
            # Compile regex once before loop (case-insensitive)
            regex = re.compile(name, re.IGNORECASE)
            for item in items:
                try:
                    value = key_func(item)
                    if value and regex.search(str(value)):
                        results.append(item)
                except (KeyError, AttributeError, TypeError):
                    # Skip items where key extraction fails
                    continue
        else:
            # Exact match
            for item in items:
                try:
                    value = key_func(item)
                    if value == name:
                        results.append(item)
                except (KeyError, AttributeError, TypeError):
                    # Skip items where key extraction fails
                    continue

        return results

    def _ensure_loaded(self):
        """Ensure documentation is loaded."""
        if not self._loaded:
            if not self.load():
                raise RuntimeError(
                    f"Documentation not found at {self.docs_path}. "
                    "Run `sdd doc generate` first to generate it."
                )

    def find_class(self, name: str, pattern: bool = False) -> List[QueryResult]:
        """
        Find class(es) by name or pattern.

        Args:
            name: Class name or regex pattern
            pattern: If True, treat name as regex pattern

        Returns:
            List of matching QueryResult objects
        """
        self._ensure_loaded()

        results = []
        classes = self.data.get('classes', [])

        if pattern:
            regex = re.compile(name, re.IGNORECASE)
            for cls in classes:
                if regex.search(cls['name']):
                    results.append(QueryResult(
                        entity_type='class',
                        name=cls['name'],
                        data=cls
                    ))
        else:
            for cls in classes:
                if cls['name'] == name:
                    results.append(QueryResult(
                        entity_type='class',
                        name=cls['name'],
                        data=cls
                    ))

        return results

    def find_function(self, name: str, pattern: bool = False) -> List[QueryResult]:
        """
        Find function(s) by name or pattern.

        Args:
            name: Function name or regex pattern
            pattern: If True, treat name as regex pattern

        Returns:
            List of matching QueryResult objects
        """
        self._ensure_loaded()

        results = []
        functions = self.data.get('functions', [])

        if pattern:
            regex = re.compile(name, re.IGNORECASE)
            for func in functions:
                if regex.search(func['name']):
                    results.append(QueryResult(
                        entity_type='function',
                        name=func['name'],
                        data=func
                    ))
        else:
            for func in functions:
                if func['name'] == name:
                    results.append(QueryResult(
                        entity_type='function',
                        name=func['name'],
                        data=func
                    ))

        return results

    def find_module(self, name: str, pattern: bool = False) -> List[QueryResult]:
        """
        Find module(s) by name or pattern.

        Args:
            name: Module name or regex pattern
            pattern: If True, treat name as regex pattern

        Returns:
            List of matching QueryResult objects
        """
        self._ensure_loaded()

        results = []

        module_keys = list(self._module_cache.keys())

        if pattern:
            regex = re.compile(name, re.IGNORECASE)
            for module in module_keys:
                if regex.search(module) or regex.search(Path(module).stem):
                    results.append(QueryResult(
                        entity_type='module',
                        name=module,
                        data=self._get_module_info(module)
                    ))
        else:
            resolved = self._resolve_module_key(name)
            if resolved in self._module_cache:
                results.append(QueryResult(
                    entity_type='module',
                    name=resolved,
                    data=self._get_module_info(resolved)
                ))
            else:
                for module in module_keys:
                    if module == name or module.endswith(name):
                        results.append(QueryResult(
                            entity_type='module',
                            name=module,
                            data=self._get_module_info(module)
                        ))

        return results

    def _get_module_info(self, module_path: str) -> Dict:
        """Get comprehensive information about a module."""
        self._ensure_loaded()
        module_key = self._resolve_module_key(module_path)
        module_info = self._module_cache.get(module_key)
        if not module_info:
            module_info = self._assemble_module_entry(module_key, None)
            self._module_cache[module_key] = module_info
        return self._copy_module_info(module_info)

    def get_high_complexity(self, threshold: int = 5,
                           module: Optional[str] = None) -> List[QueryResult]:
        """
        Get functions above complexity threshold.

        Args:
            threshold: Minimum complexity score
            module: Optional module filter

        Returns:
            List of high-complexity functions sorted by complexity (descending)
        """
        self._ensure_loaded()

        results = []
        functions = self.data.get('functions', [])

        for func in functions:
            complexity = func.get('complexity', 0)
            if complexity >= threshold:
                # Apply module filter if specified
                if module and not func.get('file', '').endswith(module):
                    continue

                results.append(QueryResult(
                    entity_type='function',
                    name=func['name'],
                    data=func,
                    relevance_score=complexity
                ))

        # Sort by complexity (descending)
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        return results

    def get_dependencies(self, module_path: str,
                        reverse: bool = False) -> List[QueryResult]:
        """
        Get dependencies for a module.

        Args:
            module_path: Path to the module
            reverse: If True, find modules that depend on this one

        Returns:
            List of dependencies
        """
        self._ensure_loaded()

        results = []
        dependencies_map = self.data.get('dependencies', {})
        module_key = self._resolve_module_key(module_path)

        if reverse:
            # Find modules that depend on this one
            for mod in self._reverse_dependencies.get(module_key, []):
                deps = dependencies_map.get(mod, [])
                results.append(QueryResult(
                    entity_type='dependency',
                    name=mod,
                    data={'depends_on': module_key, 'all_dependencies': deps}
                ))
        else:
            # Get direct dependencies of this module
            deps = dependencies_map.get(module_key, [])
            for dep in deps:
                results.append(QueryResult(
                    entity_type='dependency',
                    name=dep,
                    data={'depended_by': module_key}
                ))

        return results

    def search_entities(self, query: str) -> List[QueryResult]:
        """
        Search across all entities (classes, functions, modules).

        Args:
            query: Search query (regex pattern)

        Returns:
            List of matching entities
        """
        self._ensure_loaded()

        results = []
        regex = re.compile(query, re.IGNORECASE)

        # Search classes
        for cls in self.data.get('classes', []):
            score = 0
            if regex.search(cls['name']):
                score += 10
            if regex.search(cls.get('file', '')):
                score += 5
            if cls.get('docstring') and regex.search(cls['docstring']):
                score += 3

            if score > 0:
                results.append(QueryResult(
                    entity_type='class',
                    name=cls['name'],
                    data=cls,
                    relevance_score=score
                ))

        # Search functions
        for func in self.data.get('functions', []):
            score = 0
            if regex.search(func['name']):
                score += 10
            if regex.search(func.get('file', '')):
                score += 5
            if func.get('docstring') and regex.search(func['docstring']):
                score += 3

            if score > 0:
                results.append(QueryResult(
                    entity_type='function',
                    name=func['name'],
                    data=func,
                    relevance_score=score
                ))

        # Sort by relevance
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        return results

    def get_context_for_area(
        self,
        area_pattern: str,
        limit: Optional[int] = None,
        include_docstrings: bool = False,
        include_stats: bool = False
    ) -> Dict[str, List[QueryResult]]:
        """
        Gather all relevant context for a feature area.

        Args:
            area_pattern: Pattern to match (e.g., 'scoring', 'wizard', 'session')
            limit: Optional cap for number of results per entity type
            include_docstrings: Whether to include docstring excerpts in result data
            include_stats: Whether to include statistics payloads

        Returns:
            Dict with 'classes', 'functions', 'modules', 'dependencies' keys
        """
        self._ensure_loaded()

        context = {
            'classes': [],
            'functions': [],
            'modules': [],
            'dependencies': []
        }

        regex = re.compile(area_pattern, re.IGNORECASE)

        def apply_limit(results_list: List[QueryResult]) -> List[QueryResult]:
            if limit is not None and limit >= 0:
                return results_list[:limit]
            return results_list

        # Find matching classes
        for cls in self.data.get('classes', []):
            if regex.search(cls['name']) or regex.search(cls.get('file', '')) or (
                include_docstrings and cls.get('docstring') and regex.search(cls['docstring'])
            ):
                data = dict(cls)
                if include_docstrings:
                    data['docstring_excerpt'] = self._get_docstring_excerpt(cls.get('docstring'))
                score = 0
                if regex.search(cls['name']):
                    score += 10
                if regex.search(cls.get('file', '')):
                    score += 5
                if cls.get('docstring') and regex.search(cls['docstring']):
                    score += 3
                score = score or 1.0
                context['classes'].append(QueryResult(
                    entity_type='class',
                    name=cls['name'],
                    data=data,
                    relevance_score=score
                ))

        # Find matching functions
        for func in self.data.get('functions', []):
            docstring = func.get('docstring')
            if regex.search(func['name']) or regex.search(func.get('file', '')) or (
                include_docstrings and docstring and regex.search(docstring)
            ):
                data = dict(func)
                data['complexity'] = func.get('complexity', 0)
                data['high_complexity'] = data['complexity'] >= 5
                if include_docstrings:
                    data['docstring_excerpt'] = self._get_docstring_excerpt(func.get('docstring'))
                score = 0
                if regex.search(func['name']):
                    score += 10
                if regex.search(func.get('file', '')):
                    score += 5
                if docstring and regex.search(docstring):
                    score += 3
                score = max(score, data['complexity'], 1)
                context['functions'].append(QueryResult(
                    entity_type='function',
                    name=func['name'],
                    data=data,
                    relevance_score=score
                ))

        # Find matching modules
        modules = set()
        for cls in self.data.get('classes', []):
            if regex.search(cls.get('file', '')):
                modules.add(cls['file'])
        for func in self.data.get('functions', []):
            if regex.search(func.get('file', '')):
                modules.add(func['file'])

        for key, module in self._module_cache.items():
            docstring = module.get('docstring')
            if regex.search(key) or regex.search(module.get('name', '')) or (
                include_docstrings and docstring and regex.search(docstring)
            ):
                modules.add(key)

        for module in modules:
            resolved = self._resolve_module_key(module)
            module_record = self._module_cache.get(resolved)
            if not module_record:
                module_record = self._assemble_module_entry(resolved, None)
                self._module_cache[resolved] = module_record
            module_info = self._copy_module_info(module_record)
            if include_docstrings:
                module_info['docstring_excerpt'] = module_info.get('docstring_excerpt') or self._get_docstring_excerpt(module_info.get('docstring'))
            if not include_stats:
                module_info.pop('statistics', None)
            context['modules'].append(QueryResult(
                entity_type='module',
                name=module_info.get('file', resolved),
                data=module_info
            ))

        # Get dependencies for matching modules
        seen_dependencies = set()
        for module in modules:
            deps = self.get_dependencies(module, reverse=False)
            for dep in deps:
                key = (dep.name, dep.data.get('depended_by'))
                if key not in seen_dependencies:
                    seen_dependencies.add(key)
                    context['dependencies'].append(dep)

            reverse_deps = self.get_dependencies(module, reverse=True)
            for dep in reverse_deps:
                key = (dep.name, dep.data.get('depends_on'))
                if key not in seen_dependencies:
                    seen_dependencies.add(key)
                    context['dependencies'].append(dep)

        # Apply limit to each section if requested
        context['classes'] = apply_limit(sorted(
            context['classes'], key=lambda r: r.relevance_score, reverse=True))
        context['functions'] = apply_limit(sorted(
            context['functions'], key=lambda r: r.relevance_score, reverse=True))
        context['modules'] = apply_limit(sorted(
            context['modules'], key=lambda r: r.data.get('statistics', {}).get('avg_complexity', 0), reverse=True
        ))
        context['dependencies'] = apply_limit(sorted(
            context['dependencies'], key=lambda r: r.name
        ))

        return context

    def describe_module(
        self,
        module_path: str,
        top_functions: Optional[int] = None,
        include_docstrings: bool = True,
        include_dependencies: bool = True
    ) -> Dict[str, Any]:
        """Provide a rich summary for a given module."""
        self._ensure_loaded()
        key = self._resolve_module_key(module_path)
        module_info = self._module_cache.get(key)
        if not module_info:
            module_info = self._assemble_module_entry(key, None)
            self._module_cache[key] = module_info

        summary = self._copy_module_info(module_info)
        if not include_docstrings:
            summary.pop('docstring', None)
            summary.pop('docstring_excerpt', None)

        if not include_dependencies:
            summary.pop('dependencies', None)
            summary.pop('reverse_dependencies', None)

        if top_functions is not None:
            sorted_funcs = sorted(
                summary.get('functions', []),
                key=lambda f: f.get('complexity', 0),
                reverse=True
            )
            summary['functions'] = sorted_funcs[:top_functions]

        if include_docstrings:
            summary['classes'] = [
                {
                    **cls,
                    'docstring_excerpt': self._get_docstring_excerpt(cls.get('docstring'))
                }
                for cls in summary.get('classes', [])
            ]
            summary['functions'] = [
                {
                    **func,
                    'docstring_excerpt': self._get_docstring_excerpt(func.get('docstring'))
                }
                for func in summary.get('functions', [])
            ]

        summary['statistics'] = summary.get('statistics', {})

        return summary

    def get_stats(self) -> Dict[str, Any]:
        """
        Get documentation statistics.

        Returns:
            Dict with statistics
        """
        self._ensure_loaded()

        classes = self.data.get('classes', [])
        functions = self.data.get('functions', [])

        # Calculate complexity stats
        complexities = [f.get('complexity', 0) for f in functions]
        avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        max_complexity = max(complexities) if complexities else 0

        modules_total = len(self._module_cache) or len({
            cls.get('file') for cls in classes if cls.get('file')
        }.union({
            func.get('file') for func in functions if func.get('file')
        }))

        statistics = dict(self.data.get('statistics', {}))
        metadata = dict(self.data.get('metadata', {}))

        statistics.setdefault('total_classes', len(classes))
        statistics.setdefault('total_functions', len(functions))
        statistics.setdefault('total_modules', modules_total)
        statistics.setdefault('avg_complexity', round(avg_complexity, 2))
        statistics.setdefault('max_complexity', max_complexity)
        statistics.setdefault('high_complexity_count', len([c for c in complexities if c >= 5]))
        statistics.setdefault('high_complexity_functions', [
            func.get('name') for func in functions if func.get('complexity', 0) >= 5
        ])
        statistics.setdefault('total_files', statistics.get('total_modules', modules_total))
        statistics.setdefault('total_lines', statistics.get('total_lines', 0))

        metadata.setdefault('generated_at', self.data.get('generated_at', 'unknown'))
        metadata.setdefault('project_name', metadata.get('project_name', self.data.get('project_name', 'unknown')))
        metadata.setdefault('version', metadata.get('version', self.data.get('version', 'unknown')))

        existing_languages = metadata.get('languages')
        if isinstance(existing_languages, str):
            languages_list = [existing_languages]
        elif isinstance(existing_languages, Iterable):
            languages_list = [str(lang) for lang in existing_languages if lang]
        else:
            languages_list = []

        if not languages_list:
            language_field = metadata.get('language') or self.data.get('language')
            if language_field:
                languages_list = [str(language_field)]
            else:
                languages_list = [str(lang) for lang in self.data.get('languages', []) if lang]

        metadata['languages'] = languages_list

        return {
            'generated_at': metadata.get('generated_at', 'unknown'),
            'metadata': metadata,
            'statistics': statistics
        }

    def list_classes(self, module: Optional[str] = None) -> List[QueryResult]:
        """
        List all classes, optionally filtered by module.

        Args:
            module: Optional module filter

        Returns:
            List of classes
        """
        self._ensure_loaded()

        results = []
        for cls in self.data.get('classes', []):
            if module and not cls.get('file', '').endswith(module):
                continue
            results.append(QueryResult(
                entity_type='class',
                name=cls['name'],
                data=cls
            ))

        return results

    def list_functions(self, module: Optional[str] = None) -> List[QueryResult]:
        """
        List all functions, optionally filtered by module.

        Args:
            module: Optional module filter

        Returns:
            List of functions
        """
        self._ensure_loaded()

        results = []
        for func in self.data.get('functions', []):
            if module and not func.get('file', '').endswith(module):
                continue
            results.append(QueryResult(
                entity_type='function',
                name=func['name'],
                data=func
            ))

        return results

    def list_modules(self) -> List[QueryResult]:
        """
        List all modules.

        Returns:
            List of modules
        """
        self._ensure_loaded()

        results = []
        for module in sorted(self._module_cache.keys()):
            results.append(QueryResult(
                entity_type='module',
                name=module,
                data=self._get_module_info(module)
            ))

        return results

    def get_callers(
        self,
        function_name: str,
        include_file: bool = True,
        include_line: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Get all functions that call the specified function.

        Queries the 'callers' field from schema v2.0 to return functions
        that call the specified function. This enables "who calls this?" queries.

        Args:
            function_name: Name of the function to query
            include_file: Include file path in results (default: True)
            include_line: Include line numbers in results (default: True)

        Returns:
            List of caller information dictionaries with keys:
            - name: Name of the calling function
            - call_type: Type of call (e.g., 'function_call', 'method_call')
            - file: File path (if include_file=True)
            - line: Line number (if include_line=True)

        Example:
            >>> query = DocumentationQuery()
            >>> query.load()
            >>> callers = query.get_callers("process_data")
            >>> for caller in callers:
            ...     print(f"{caller['name']} calls process_data at {caller['file']}:{caller['line']}")

        Note:
            Returns empty list if function not found or if using schema v1.0
            documentation without cross-reference data.
        """
        self._ensure_loaded()

        # Find the function
        functions = [f for f in self.data.get('functions', [])
                     if f.get('name') == function_name]

        if not functions:
            return []

        # Get callers from the first matching function
        func = functions[0]
        callers_data = func.get('callers', [])

        # Format results
        results = []
        for caller in callers_data:
            if not isinstance(caller, dict):
                continue

            result = {
                'name': caller.get('name', ''),
                'call_type': caller.get('call_type', 'unknown')
            }

            if include_file:
                result['file'] = caller.get('file', '')

            if include_line:
                result['line'] = caller.get('line')

            results.append(result)

        return results

    def get_callees(
        self,
        function_name: str,
        include_file: bool = True,
        include_line: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Get all functions called by the specified function.

        Queries the 'calls' field from schema v2.0 to return functions
        called by the specified function. This enables "what does this call?" queries.

        Args:
            function_name: Name of the function to query
            include_file: Include file path in results (default: True)
            include_line: Include line numbers in results (default: True)

        Returns:
            List of callee information dictionaries with keys:
            - name: Name of the called function
            - call_type: Type of call (e.g., 'function_call', 'method_call')
            - file: File path (if include_file=True)
            - line: Line number (if include_line=True)

        Example:
            >>> query = DocumentationQuery()
            >>> query.load()
            >>> callees = query.get_callees("main")
            >>> for callee in callees:
            ...     print(f"main calls {callee['name']} at {callee['file']}:{callee['line']}")

        Note:
            Returns empty list if function not found or if using schema v1.0
            documentation without cross-reference data.
        """
        self._ensure_loaded()

        # Find the function
        functions = [f for f in self.data.get('functions', [])
                     if f.get('name') == function_name]

        if not functions:
            return []

        # Get calls from the first matching function
        func = functions[0]
        calls_data = func.get('calls', [])

        # Format results
        results = []
        for callee in calls_data:
            if not isinstance(callee, dict):
                continue

            result = {
                'name': callee.get('name', ''),
                'call_type': callee.get('call_type', 'unknown')
            }

            if include_file:
                result['file'] = callee.get('file', '')

            if include_line:
                result['line'] = callee.get('line')

            results.append(result)

        return results

    def get_call_count(self, function_name: str) -> Optional[int]:
        """
        Get the total number of times a function is called across the codebase.

        Returns the call_count field from schema v2.0 if available.

        Args:
            function_name: Name of the function to query

        Returns:
            Call count as an integer, or None if:
            - Function not found
            - Using schema v1.0 without call_count field
            - Call count not computed

        Example:
            >>> query = DocumentationQuery()
            >>> query.load()
            >>> count = query.get_call_count("process_data")
            >>> if count is not None:
            ...     print(f"process_data is called {count} times")
        """
        self._ensure_loaded()

        functions = [f for f in self.data.get('functions', [])
                     if f.get('name') == function_name]

        if not functions:
            return None

        return functions[0].get('call_count')

    def _create_graph_node(
        self,
        function_name: str,
        depth: int,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """
        Create a graph node with function metadata.

        Helper method for build_call_graph() that creates a node
        dictionary with function information.

        Args:
            function_name: Name of the function
            depth: Depth in the graph from the root
            include_metadata: Include file path and metadata

        Returns:
            Dictionary with node data
        """
        node = {
            "name": function_name,
            "depth": depth
        }

        if include_metadata:
            # Find function in data
            functions = [f for f in self.data.get('functions', [])
                         if f.get('name') == function_name]

            if functions:
                func = functions[0]
                node['file'] = func.get('file', '')
                node['line'] = func.get('line')
                call_count = func.get('call_count')
                if call_count is not None:
                    node['call_count'] = call_count

        return node

    def build_call_graph(
        self,
        function_name: str,
        direction: str = "both",
        max_depth: int = 3,
        include_metadata: bool = True
    ) -> Dict[str, Any]:
        """
        Build a call graph starting from the specified function.

        Recursively traverses function call relationships to build a complete
        graph of callers, callees, or both. Useful for impact analysis and
        visualizing function dependencies.

        Uses breadth-first search (BFS) to explore the graph, tracking visited
        nodes to handle cycles and respecting max_depth to prevent excessive
        recursion.

        Args:
            function_name: Starting function for the graph
            direction: Direction to traverse (default: "both"):
                - "callers": Functions that call this function (upstream)
                - "callees": Functions called by this function (downstream)
                - "both": Both directions (full dependency graph)
            max_depth: Maximum recursion depth (default: 3).
                       Depth 0 is the root function.
            include_metadata: Include file paths and metadata in nodes
                             (default: True)

        Returns:
            Dictionary with keys:
            - root: Starting function name
            - direction: Direction traversed
            - max_depth: Maximum depth used
            - nodes: Dict mapping function names to node data
            - edges: List of call relationships (from -> to)
            - truncated: True if max_depth was reached (graph incomplete)

        Example:
            >>> query = DocumentationQuery()
            >>> query.load()
            >>> graph = query.build_call_graph("process_data", direction="both", max_depth=2)
            >>> print(f"Graph has {len(graph['nodes'])} nodes and {len(graph['edges'])} edges")
            Graph has 5 nodes and 6 edges
            >>> for edge in graph['edges']:
            ...     print(f"{edge['from']} -> {edge['to']}")
            main -> process_data
            process_data -> validate
            process_data -> save

        Note:
            - Handles cycles by tracking visited nodes (won't revisit)
            - Respects max_depth to prevent excessive recursion
            - Returns empty graph if function not found
            - Works with schema v2.0 cross-reference data
            - Returns minimal graph for v1.0 docs (no cross-references)
        """
        self._ensure_loaded()

        # Validate direction parameter
        if direction not in ["callers", "callees", "both"]:
            raise ValueError(f"Invalid direction: {direction}. Must be 'callers', 'callees', or 'both'")

        # Initialize graph structure
        graph: Dict[str, Any] = {
            "root": function_name,
            "direction": direction,
            "max_depth": max_depth,
            "nodes": {},
            "edges": [],
            "truncated": False
        }

        # Check if function exists
        functions = [f for f in self.data.get('functions', [])
                     if f.get('name') == function_name]

        if not functions:
            # Function not found - return empty graph
            return graph

        # Add root node
        root_node = self._create_graph_node(function_name, 0, include_metadata)
        graph["nodes"][function_name] = root_node

        # BFS traversal
        queue = [(function_name, 0)]  # (function_name, depth)
        visited = {function_name}

        while queue:
            current_func, current_depth = queue.pop(0)

            # Check if we've reached max depth
            if current_depth >= max_depth:
                graph["truncated"] = True
                continue

            # Get neighbors based on direction
            neighbors = []

            # Upstream: who calls this function?
            if direction in ["callers", "both"]:
                callers = self.get_callers(current_func, include_file=True, include_line=True)
                for caller in callers:
                    caller_name = caller['name']
                    neighbors.append(caller_name)

                    # Add edge: caller -> current_func
                    edge = {
                        "from": caller_name,
                        "to": current_func,
                        "type": "calls",
                        "call_type": caller.get('call_type', 'unknown')
                    }
                    if edge not in graph["edges"]:
                        graph["edges"].append(edge)

            # Downstream: what does this function call?
            if direction in ["callees", "both"]:
                callees = self.get_callees(current_func, include_file=True, include_line=True)
                for callee in callees:
                    callee_name = callee['name']
                    neighbors.append(callee_name)

                    # Add edge: current_func -> callee
                    edge = {
                        "from": current_func,
                        "to": callee_name,
                        "type": "calls",
                        "call_type": callee.get('call_type', 'unknown')
                    }
                    if edge not in graph["edges"]:
                        graph["edges"].append(edge)

            # Add unvisited neighbors to queue
            for neighbor_name in neighbors:
                if neighbor_name not in visited:
                    visited.add(neighbor_name)
                    # Create and add node
                    node = self._create_graph_node(neighbor_name, current_depth + 1, include_metadata)
                    graph["nodes"][neighbor_name] = node
                    # Add to queue for further exploration
                    queue.append((neighbor_name, current_depth + 1))

        return graph


def check_docs_exist(docs_path: Optional[str] = None) -> bool:
    """
    Check if documentation files exist.

    Args:
        docs_path: Path to docs directory or codebase.json

    Returns:
        True if documentation exists
    """
    if docs_path is None:
        docs_path = Path.cwd() / "docs"
    else:
        docs_path = Path(docs_path)

    if docs_path.is_dir():
        docs_path = docs_path / "codebase.json"

    return docs_path.exists()


def check_documentation_staleness(
    docs_path: Optional[str] = None,
    source_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    Check if documentation is stale by comparing generation time with source file modifications.

    Args:
        docs_path: Path to docs directory or codebase.json
        source_dir: Path to source directory to check. If None, uses parent of docs directory.

    Returns:
        Dictionary with staleness information:
        {
            'is_stale': bool,
            'docs_generated_at': str (ISO timestamp),
            'latest_source_modification': str (ISO timestamp),
            'message': str (human-readable message),
            'docs_age_seconds': int,
            'newest_file': str (path to newest source file),
            'checked_files_count': int
        }
    """
    from datetime import datetime, timezone, timedelta
    import os
    import time

    # Resolve docs path
    if docs_path is None:
        docs_path = Path.cwd() / "docs"
    else:
        docs_path = Path(docs_path)

    if docs_path.is_dir():
        docs_path = docs_path / "codebase.json"

    # Check if docs exist
    if not docs_path.exists():
        return {
            'is_stale': True,
            'docs_generated_at': None,
            'latest_source_modification': None,
            'message': 'Documentation does not exist',
            'docs_age_seconds': None,
            'newest_file': None,
            'checked_files_count': 0,
            'error': 'Documentation not found'
        }

    # Load docs to get generation time
    try:
        with open(docs_path, 'r') as f:
            docs_data = json.load(f)
    except Exception as e:
        return {
            'is_stale': True,
            'docs_generated_at': None,
            'latest_source_modification': None,
            'message': f'Error loading documentation: {e}',
            'docs_age_seconds': None,
            'newest_file': None,
            'checked_files_count': 0,
            'error': str(e)
        }

    # Get generation timestamp
    generated_at_str = docs_data.get('metadata', {}).get('generated_at')
    if not generated_at_str:
        return {
            'is_stale': True,
            'docs_generated_at': None,
            'latest_source_modification': None,
            'message': 'Documentation metadata missing generation timestamp',
            'docs_age_seconds': None,
            'newest_file': None,
            'checked_files_count': 0,
            'error': 'Missing timestamp'
        }

    # Parse generation timestamp
    try:
        generated_at = datetime.fromisoformat(generated_at_str.replace('Z', '+00:00'))
    except Exception as e:
        return {
            'is_stale': True,
            'docs_generated_at': generated_at_str,
            'latest_source_modification': None,
            'message': f'Invalid timestamp format: {e}',
            'docs_age_seconds': None,
            'newest_file': None,
            'checked_files_count': 0,
            'error': str(e)
        }

    # Determine source directory to scan
    if source_dir is None:
        # Default: scan parent directory of docs
        source_dir = docs_path.parent.parent
    else:
        source_dir = Path(source_dir)

    if not source_dir.exists():
        return {
            'is_stale': False,
            'docs_generated_at': generated_at_str,
            'latest_source_modification': None,
            'message': f'Source directory not found: {source_dir}',
            'docs_age_seconds': 0,
            'newest_file': None,
            'checked_files_count': 0,
            'error': 'Source directory not found'
        }

    # Scan source directory for Python files
    latest_mtime = 0
    newest_file = None
    checked_count = 0

    # File extensions to check based on documentation languages
    languages = docs_data.get('metadata', {}).get('languages', ['python'])
    extensions = set()
    for lang in languages:
        if lang == 'python':
            extensions.add('.py')
        elif lang == 'javascript' or lang == 'typescript':
            extensions.update(['.js', '.jsx', '.ts', '.tsx'])
        elif lang == 'go':
            extensions.add('.go')
        elif lang == 'html':
            extensions.add('.html')
        elif lang == 'css':
            extensions.add('.css')

    # If no extensions found, default to Python
    if not extensions:
        extensions.add('.py')

    # Scan recursively
    try:
        for root, dirs, files in os.walk(source_dir):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if d not in {
                '__pycache__', '.git', 'node_modules', 'venv', '.venv',
                'build', 'dist', '.pytest_cache', '.mypy_cache', 'docs'
            }]

            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    file_path = Path(root) / file
                    try:
                        mtime = file_path.stat().st_mtime
                        checked_count += 1
                        if mtime > latest_mtime:
                            latest_mtime = mtime
                            newest_file = str(file_path)
                    except Exception:
                        # Skip files we can't stat
                        continue
    except Exception as e:
        return {
            'is_stale': False,
            'docs_generated_at': generated_at_str,
            'latest_source_modification': None,
            'message': f'Error scanning source directory: {e}',
            'docs_age_seconds': 0,
            'newest_file': None,
            'checked_files_count': 0,
            'error': str(e)
        }

    if checked_count == 0:
        return {
            'is_stale': False,
            'docs_generated_at': generated_at_str,
            'latest_source_modification': None,
            'message': f'No source files found in {source_dir}',
            'docs_age_seconds': 0,
            'newest_file': None,
            'checked_files_count': 0
        }

    # Compare timestamps
    latest_source_dt = datetime.fromtimestamp(latest_mtime, tz=timezone.utc)

    # Make generated_at timezone-aware if needed
    # Naive timestamps from datetime.now() are in local time, not UTC
    if generated_at.tzinfo is None:
        # Assume it's local time and convert to UTC
        local_offset = time.timezone if time.daylight == 0 else time.altzone
        generated_at = generated_at.replace(tzinfo=timezone(timedelta(seconds=-local_offset)))
        # Convert to UTC for comparison
        generated_at = generated_at.astimezone(timezone.utc)

    is_stale = latest_source_dt > generated_at

    # Calculate age
    now = datetime.now(timezone.utc)
    docs_age = now - generated_at
    docs_age_seconds = int(docs_age.total_seconds())

    # Format message
    if is_stale:
        time_diff = latest_source_dt - generated_at
        if time_diff.days > 0:
            time_str = f"{time_diff.days} day{'s' if time_diff.days > 1 else ''}"
        elif time_diff.seconds >= 3600:
            hours = time_diff.seconds // 3600
            time_str = f"{hours} hour{'s' if hours > 1 else ''}"
        elif time_diff.seconds >= 60:
            minutes = time_diff.seconds // 60
            time_str = f"{minutes} minute{'s' if minutes > 1 else ''}"
        else:
            time_str = f"{time_diff.seconds} second{'s' if time_diff.seconds > 1 else ''}"

        message = f"Documentation is stale (generated {_format_age(docs_age_seconds)} ago, source modified {time_str} after generation)"
    else:
        message = f"Documentation is fresh (generated {_format_age(docs_age_seconds)} ago)"

    return {
        'is_stale': is_stale,
        'docs_generated_at': generated_at_str,
        'latest_source_modification': latest_source_dt.isoformat(),
        'message': message,
        'docs_age_seconds': docs_age_seconds,
        'newest_file': newest_file,
        'checked_files_count': checked_count
    }


def _format_age(seconds: int) -> str:
    """Format age in seconds as human-readable string."""
    if seconds < 60:
        return f"{seconds} second{'s' if seconds != 1 else ''}"
    elif seconds < 3600:
        minutes = seconds // 60
        return f"{minutes} minute{'s' if minutes != 1 else ''}"
    elif seconds < 86400:
        hours = seconds // 3600
        return f"{hours} hour{'s' if hours != 1 else ''}"
    else:
        days = seconds // 86400
        return f"{days} day{'s' if days != 1 else ''}"


def load_documentation(docs_path: Optional[str] = None) -> DocumentationQuery:
    """
    Convenience function to load documentation.

    Args:
        docs_path: Path to docs directory or codebase.json

    Returns:
        Loaded DocumentationQuery object
    """
    query = DocumentationQuery(docs_path)
    query.load()
    return query

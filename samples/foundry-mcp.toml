# foundry-mcp Configuration Sample
#
# This file demonstrates all available configuration options.
# Copy to your project root as `foundry-mcp.toml` and customize.
#
# Configuration Priority (highest to lowest):
# 1. Environment variables
# 2. TOML config file
# 3. Default values

# =============================================================================
# Workspace Configuration
# =============================================================================

[workspace]
# Path to specs directory (auto-detected if not set)
specs_dir = "./specs"

# Additional workspace roots for multi-project setups
# roots = ["./project-a", "./project-b"]

# Path to journals directory (defaults to specs_dir/journals)
# journals_path = "./specs/journals"

# =============================================================================
# Logging Configuration
# =============================================================================

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Enable structured JSON logging (recommended for production)
structured = true

# =============================================================================
# Authentication Configuration
# =============================================================================

[auth]
# Require API key authentication for all requests
require_auth = false

# Valid API keys (rotate regularly, recommended: every 90 days)
# api_keys = ["key1", "key2"]

# =============================================================================
# Server Configuration
# =============================================================================

[server]
name = "foundry-mcp"
version = "0.2.0"

# =============================================================================
# LLM Provider Configuration (Legacy - see [consultation] for new priority system)
# =============================================================================

[llm]
# NOTE: The [llm] section is being deprecated in favor of [consultation].priority
# which provides unified API/CLI provider configuration with model selection.
# See the [consultation] section below for the recommended configuration.

# Provider type: "openai", "anthropic", or "local"
provider = "openai"

# API key (optional - falls back to environment variable)
# Priority: config api_key > FOUNDRY_MCP_LLM_API_KEY > provider-specific env var
# api_key = "sk-..."

# Model identifier (uses provider default if not set)
# OpenAI default: "gpt-4"
# Anthropic default: "claude-sonnet-4-20250514"
# Local default: "llama3.2"
# model = "gpt-4"

# Request timeout in seconds
timeout = 30

# Custom API base URL (for proxies, Azure OpenAI, or local servers)
# base_url = "https://api.openai.com/v1"

# Organization ID (OpenAI only)
# organization = "org-..."

# Default max tokens for responses
max_tokens = 1024

# Default temperature for generation (0.0 - 2.0)
# Lower = more deterministic, Higher = more creative
temperature = 0.7

# =============================================================================
# Workflow Configuration
# =============================================================================

[workflow]
# Execution mode:
#   "single"     - One task at a time with user approval
#   "autonomous" - Complete all phase tasks automatically
#   "batch"      - Execute batch_size tasks, then pause
mode = "single"

# Automatically run validation after task completion
auto_validate = true

# Enable journaling of task completions
journal_enabled = true

# Number of tasks to execute in batch mode
batch_size = 5

# Context usage threshold (%) to trigger automatic pause
# When context reaches this threshold, autonomous/batch mode pauses
context_threshold = 85

# =============================================================================
# AI Consultation Configuration
# =============================================================================

[consultation]
# =============================================================================
# Provider Priority Configuration (NEW - Unified API/CLI providers)
# =============================================================================
#
# Provider priority list - first available provider wins.
# Supports both API providers and CLI tools with model selection.
#
# Format: "[api]provider/model" or "[cli]transport[:backend/model|:model]"
#
# Examples:
#   [api]openai/gpt-4.1              - Direct OpenAI API with gpt-4.1
#   [api]anthropic/claude-sonnet-4   - Direct Anthropic API with Claude
#   [cli]gemini:auto                  - Gemini CLI with pro model
#   [cli]claude:opus                 - Claude CLI with opus model
#   [cli]opencode:openai/gpt-5.2     - opencode CLI routing to OpenAI backend
#   [cli]opencode:gemini/gemini-2.5  - opencode CLI routing to Gemini backend
#   [cli]codex                       - Codex CLI with default model
#
# priority = [
#     "[cli]gemini:auto",
#     "[cli]claude:opus",
#     "[cli]opencode:openai/gpt-5.2",
#     "[api]openai/gpt-4.1",
#     "[api]anthropic/claude-sonnet-4",
# ]

# Per-provider overrides (optional)
# Override timeout, temperature, or other settings for specific providers
# [consultation.overrides]
# "[cli]opencode:openai/gpt-5.2" = { timeout = 600 }
# "[api]openai/gpt-4.1" = { temperature = 0.3 }

# =============================================================================
# Operational Settings
# =============================================================================

# Default timeout for AI consultations in seconds
# Increased from 120s to handle longer fidelity reviews
default_timeout = 300

# Number of retry attempts per provider for transient failures
# (timeouts, rate limits, 5xx errors)
max_retries = 2

# Delay between retry attempts in seconds
retry_delay = 5.0

# Enable fallback to next available provider when one fails
# Set to false to only use the requested/first provider
fallback_enabled = true

# Cache time-to-live in seconds for consultation results
cache_ttl = 3600

# =============================================================================
# Per-Workflow Consultation Configuration
# =============================================================================
#
# Override consultation settings for specific workflows.
# Each workflow can specify minimum model requirements for consensus
# and optional timeout overrides.
#
# Available settings:
#   min_models      - Minimum models required for consensus (default: 1)
#   timeout_override - Override default_timeout for this workflow (optional)

# --- Fidelity Review Workflow ---
# Requires 2 models for consensus to catch implementation deviations
[consultation.workflows.fidelity_review]
min_models = 2
timeout_override = 600.0  # Longer timeout for comprehensive code analysis

# --- Plan Review Workflow ---
# Requires 3 models for consensus on architectural decisions
[consultation.workflows.plan_review]
min_models = 3
# Uses default_timeout (no override)

# --- Quick Check Workflow (example of minimal config) ---
# [consultation.workflows.quick_check]
# min_models = 1  # Single model is sufficient for quick validation

# =============================================================================
# Observability Configuration
# =============================================================================
#
# Requires optional dependencies:
#   - For OpenTelemetry: pip install foundry-mcp[tracing]
#   - For Prometheus: pip install foundry-mcp[metrics]
#   - For both: pip install foundry-mcp[observability]
#
# When dependencies are not installed, all observability features gracefully
# degrade to no-ops with zero performance overhead.

[observability]
# Master switch for all observability features
# Set to true to enable, then configure individual providers below
enabled = false

# OpenTelemetry tracing configuration
otel_enabled = false
otel_endpoint = "localhost:4317"    # OTLP gRPC endpoint
otel_service_name = "foundry-mcp"   # Service name in traces
otel_sample_rate = 1.0              # Sampling rate: 0.0 (none) to 1.0 (all)

# Prometheus metrics configuration
prometheus_enabled = false
prometheus_port = 0                  # HTTP port for /metrics (0 = no server)
prometheus_host = "0.0.0.0"          # HTTP server bind address
prometheus_namespace = "foundry_mcp" # Metric name prefix

# =============================================================================
# Observability Examples
# =============================================================================

# --- Enable OpenTelemetry only ---
# [observability]
# enabled = true
# otel_enabled = true
# otel_endpoint = "localhost:4317"
# otel_service_name = "my-foundry-mcp"
# otel_sample_rate = 0.1  # Sample 10% of traces in production

# --- Enable Prometheus metrics with HTTP server ---
# [observability]
# enabled = true
# prometheus_enabled = true
# prometheus_port = 9090  # Expose /metrics on port 9090

# --- Enable both for full observability stack ---
# [observability]
# enabled = true
# otel_enabled = true
# otel_endpoint = "otel-collector:4317"
# prometheus_enabled = true
# prometheus_port = 9090

# =============================================================================
# Git Workflow Configuration
# =============================================================================

[git]
# Enable git-aware workflows (automatic commit prompts, commit cadence, etc.)
enabled = false

# Determine when to offer automatic commits: "manual", "task", or "phase"
commit_cadence = "manual"

# Control automated behaviors (all default to false for safety)
auto_branch = false
auto_commit = false
auto_push = false
auto_pr = false

# Show staged file preview before committing (recommended)
show_before_commit = true

# =============================================================================
# Provider Priority Examples
# =============================================================================

# --- Example 1: Prefer CLI tools, fallback to API ---
# [consultation]
# priority = [
#     "[cli]gemini:auto",                       # First choice: Gemini CLI
#     "[cli]claude:opus",                      # Second: Claude CLI
#     "[cli]opencode:openai/gpt-5.2",          # Third: opencode with OpenAI
#     "[api]openai/gpt-4.1",                   # Fallback: direct API
# ]

# --- Example 2: API-only configuration ---
# [consultation]
# priority = [
#     "[api]anthropic/claude-sonnet-4",        # Prefer Anthropic
#     "[api]openai/gpt-4.1",                   # Fallback to OpenAI
# ]

# --- Example 3: Single CLI provider with backend routing ---
# [consultation]
# priority = [
#     "[cli]opencode:gemini/gemini-2.5-pro",   # Use opencode with Gemini backend
# ]

# --- Example 4: Mixed with per-provider overrides ---
# [consultation]
# priority = [
#     "[cli]opencode:openai/gpt-5.2",
#     "[api]openai/gpt-4.1",
# ]
# [consultation.overrides]
# "[cli]opencode:openai/gpt-5.2" = { timeout = 600, temperature = 0.3 }
# "[api]openai/gpt-4.1" = { timeout = 120, max_tokens = 4096 }

# =============================================================================
# Dashboard Configuration
# =============================================================================
#
# Built-in web dashboard for viewing errors, metrics, and AI provider status.
# Requires optional dependency: pip install foundry-mcp[dashboard]
#
# When aiohttp is not installed, dashboard features gracefully degrade to no-ops.

[dashboard]
# Enable the built-in web dashboard
enabled = false

# HTTP server port
port = 8080

# HTTP server bind address (127.0.0.1 for localhost-only, 0.0.0.0 for all interfaces)
host = "127.0.0.1"

# Automatically open browser when dashboard starts
auto_open_browser = false

# Frontend auto-refresh interval in milliseconds
refresh_interval_ms = 5000

# =============================================================================
# Dashboard Examples
# =============================================================================

# --- Enable dashboard with default settings ---
# [dashboard]
# enabled = true

# --- Enable with auto-open browser ---
# [dashboard]
# enabled = true
# auto_open_browser = true

# --- Custom port and faster refresh ---
# [dashboard]
# enabled = true
# port = 9000
# refresh_interval_ms = 2000

# =============================================================================
# Legacy [llm] Examples (deprecated - use [consultation].priority instead)
# =============================================================================

# --- OpenAI Configuration ---
# [llm]
# provider = "openai"
# model = "gpt-4-turbo"
# timeout = 60
# max_tokens = 2048
# temperature = 0.5
# # API key via OPENAI_API_KEY environment variable

# --- Anthropic Configuration ---
# [llm]
# provider = "anthropic"
# model = "claude-3-opus-20240229"
# timeout = 90
# max_tokens = 4096
# temperature = 0.7
# # API key via ANTHROPIC_API_KEY environment variable

# --- Local (Ollama) Configuration ---
# [llm]
# provider = "local"
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# timeout = 120
# max_tokens = 2048
# temperature = 0.8
# # No API key required for local providers

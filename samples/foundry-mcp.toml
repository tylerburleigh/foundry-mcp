# foundry-mcp Configuration Sample
#
# This file demonstrates all available configuration options.
# Copy to your project root as `foundry-mcp.toml` and customize.
#
# Configuration Priority (highest to lowest):
# 1. Environment variables
# 2. TOML config file
# 3. Default values

# =============================================================================
# Workspace Configuration
# =============================================================================

[workspace]
# Path to specs directory (auto-detected if not set)
specs_dir = "./specs"

# Additional workspace roots for multi-project setups
# roots = ["./project-a", "./project-b"]

# Path to journals directory (defaults to specs_dir/journals)
# journals_path = "./specs/journals"

# Path to bikelane intake queue storage (defaults to specs_dir/.bikelane)
# Env var: FOUNDRY_MCP_BIKELANE_DIR
# bikelane_dir = "./specs/.bikelane"

# =============================================================================
# Logging Configuration
# =============================================================================

[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Enable structured JSON logging (recommended for production)
structured = true

# =============================================================================
# Tools Configuration
# =============================================================================

[tools]
# Disable specific tools to reduce context window usage
# Tool descriptions are loaded into Claude's context on each message,
# so disabling unused tools can significantly reduce token consumption.
#
# Available tools:
#   health      - Liveness/readiness checks (used by setup)
#   plan        - Plan creation and review workflows
#   pr          - Pull request creation
#   error       - Error collection and querying
#   journal     - Implementation journals
#   authoring   - Spec authoring operations
#   review      - Fidelity and code reviews
#   spec        - Spec management
#   task        - Task management
#   provider    - AI provider status
#   environment - Environment setup and detection
#   lifecycle   - Spec lifecycle transitions
#   verification - Verification workflows
#   server      - Server introspection
#   test        - Test runner integration
#   research    - Research workflows (chat, consensus, thinkdeep, ideate, deep)
#
# Example: disable tools only used during setup (default is empty list - all tools enabled)
# disabled_tools = ["error", "health", "environment"]

# Environment variable alternative: FOUNDRY_MCP_DISABLED_TOOLS (comma-separated)
# Example: FOUNDRY_MCP_DISABLED_TOOLS=error,research

# =============================================================================
# Authentication Configuration
# =============================================================================

[auth]
# Require API key authentication for all requests
require_auth = false

# Valid API keys (rotate regularly, recommended: every 90 days)
# api_keys = ["key1", "key2"]

# =============================================================================
# Observability Configuration
# =============================================================================
#
# Requires optional dependencies:
#   - For OpenTelemetry: pip install foundry-mcp[tracing]
#   - For Prometheus: pip install foundry-mcp[metrics]
#   - For both: pip install foundry-mcp[observability]
#
# When dependencies are not installed, all observability features gracefully
# degrade to no-ops with zero performance overhead.

[observability]
# Master switch for all observability features
# Set to true to enable, then configure individual providers below
enabled = false

# OpenTelemetry tracing configuration
otel_enabled = false
otel_endpoint = "localhost:4317"    # OTLP gRPC endpoint
otel_service_name = "foundry-mcp"   # Service name in traces
otel_sample_rate = 1.0              # Sampling rate: 0.0 (none) to 1.0 (all)

# Prometheus metrics configuration
prometheus_enabled = false
prometheus_port = 0                  # HTTP port for /metrics (0 = no server)
prometheus_host = "0.0.0.0"          # HTTP server bind address
prometheus_namespace = "foundry_mcp" # Metric name prefix

# =============================================================================
# Implement Command Configuration
# =============================================================================
#
# Default flags for the /implement command. These can be overridden via CLI flags.
# The implement skill uses these settings when determining execution mode.

[implement]
# Skip user prompts between tasks (autonomous execution)
auto = false

# Use subagent(s) for implementation (recommended for better focus)
delegate = true

# Run subagents concurrently for independent tasks (implies delegate=true)
parallel = false

# =============================================================================
# Git Workflow Configuration
# =============================================================================

[git]
# Enable git-aware workflows (automatic commit prompts, commit cadence, etc.)
enabled = false

# Determine when to offer automatic commits: "manual", "task", or "phase"
commit_cadence = "manual"

# Control automated behaviors (all default to false for safety)
auto_commit = false
auto_push = false
auto_pr = false

# Show staged file preview before committing (recommended)
show_before_commit = true

# =============================================================================
# Workflow Configuration
# =============================================================================

[workflow]
# Execution mode:
#   "single"     - One task at a time with user approval
#   "autonomous" - Complete all phase tasks automatically
#   "batch"      - Execute batch_size tasks, then pause
mode = "single"

# Automatically run validation after task completion
auto_validate = true

# Enable journaling of task completions
journal_enabled = true

# Number of tasks to execute in batch mode
batch_size = 5

# Context usage threshold (%) to trigger automatic pause
# When context reaches this threshold, autonomous/batch mode pauses
context_threshold = 85

# =============================================================================
# AI Consultation Configuration
# =============================================================================

[consultation]
# Default timeout for AI CLI provider calls in seconds
# Minimum recommended: 360s for per-provider operations
default_timeout = 360

# Number of retry attempts per provider for transient failures
max_retries = 2

# Delay between retry attempts in seconds
retry_delay = 5.0

# Enable fallback to next available provider when one fails
fallback_enabled = true

# Cache time-to-live in seconds for consultation results
cache_ttl = 3600

# Provider priority list - first available provider wins
# Uncomment and customize:
# priority = [
#   "[cli]gemini:pro",
#   "[cli]codex:gpt-5.2-codex",
#   "[cli]cursor-agent:gpt-5.1-codex",
#   "[cli]opencode:openai/gpt-5.2-codex",
#   "[cli]claude:opus",
# ]

# Per-workflow overrides
[consultation.workflows.fidelity_review]
min_models = 2
timeout_override = 600.0
default_review_type = "full"

[consultation.workflows.plan_review]
min_models = 3
default_review_type = "full"

[consultation.workflows.markdown_plan_review]
min_models = 2
timeout_override = 600.0
default_review_type = "full"

# =============================================================================
# Error Collection Configuration
# =============================================================================
#
# Stores error logs for observability and debugging.
# Errors are stored in append-only JSONL format with automatic cleanup.

[error_collection]
# Enable error collection (recommended)
enabled = true

# Storage path (default: ~/.foundry-mcp/errors)
# Uncomment to customize:
# storage_path = "~/.foundry-mcp/errors"

# Delete records older than this many days
retention_days = 30

# Maximum number of error records to keep
max_errors = 10000

# Include stack traces in error records (useful for debugging)
include_stack_traces = true

# Redact sensitive data from input parameters
redact_inputs = true

# =============================================================================
# Research Workflow Configuration
# =============================================================================

[research]
# Enable research tools (chat, consensus, thinkdeep, ideate, deep-research)
enabled = true

# Default LLM provider for research workflows
# Supports ProviderSpec format: "[cli]gemini:pro", "[api]openai/gpt-4.1", or simple: "gemini"
default_provider = "[cli]gemini:pro"

# Storage path for research state (default: ~/.foundry-mcp/research)
# storage_path = "~/.foundry-mcp/research"

# State TTL in hours before cleanup
ttl_hours = 24

# Maximum messages per conversation thread
max_messages_per_thread = 100

# Default timeout for AI CLI provider calls in seconds
# Minimum recommended: 360s for per-provider operations
default_timeout = 360

# Per-provider timeout for consensus workflow (seconds)
timeout_per_provider = 360

# Per-operation timeout for deep research workflow (seconds)
timeout_per_operation = 360

# Providers for CONSENSUS workflow (multi-model consultation)
# Supports mixed notation - simple IDs and ProviderSpec both work
consensus_providers = [
    "[cli]gemini:pro",
    "[cli]codex:gpt-5.2-codex",
    "[cli]cursor-agent:gpt-5.1-codex",
    "[cli]opencode:openai/gpt-5.2-codex",
    "[cli]claude:opus",
]

# Maximum investigation depth for THINKDEEP workflow
thinkdeep_max_depth = 5

# Perspectives for IDEATE brainstorming
ideate_perspectives = ["technical", "creative", "practical", "visionary"]

# -----------------------------------------------------------------------------
# Deep Research Settings
# -----------------------------------------------------------------------------

# Maximum refinement iterations
deep_research_max_iterations = 3

# Maximum sub-queries per decomposition
deep_research_max_sub_queries = 5

# Maximum sources per sub-query
deep_research_max_sources = 5

# Follow and extract content from URLs
deep_research_follow_links = true

# Whole workflow timeout in seconds (recommended: 600s)
deep_research_timeout = 600.0

# Maximum parallel operations
deep_research_max_concurrent = 3

# Write audit artifacts for debugging
deep_research_audit_artifacts = true

# Research mode: controls source prioritization
# - "general"   : No domain preferences (default)
# - "academic"  : Prioritizes journals, publishers, preprints
# - "technical" : Prioritizes official docs, arxiv, Stack Overflow
deep_research_mode = "general"

# Search providers (in priority order)
# Available: tavily, perplexity, semantic_scholar
deep_research_providers = ["tavily", "perplexity", "semantic_scholar"]

# -----------------------------------------------------------------------------
# Per-Phase Timeouts (override deep_research_timeout)
# Minimum recommended: 360s per operation for AI CLI providers
# -----------------------------------------------------------------------------

deep_research_planning_timeout = 360.0    # Query decomposition
deep_research_analysis_timeout = 360.0    # Finding extraction
deep_research_synthesis_timeout = 600.0   # Report generation (may take longer)
deep_research_refinement_timeout = 360.0  # Gap identification

# -----------------------------------------------------------------------------
# Per-Phase Providers (override default_provider)
# -----------------------------------------------------------------------------
# Supports ProviderSpec format for model selection:
#   "[cli]gemini:pro"           - Gemini CLI with pro model
#   "[cli]claude:opus"          - Claude CLI with opus model
#   "[cli]opencode:openai/gpt-5.2" - opencode routing to OpenAI
#   "[api]openai/gpt-4.1"       - Direct OpenAI API

# deep_research_planning_provider = "[cli]gemini:pro"
# deep_research_analysis_provider = "[cli]gemini:pro"
# deep_research_synthesis_provider = "[cli]claude:opus"  # Claude for report writing
# deep_research_refinement_provider = "[cli]gemini:pro"

# -----------------------------------------------------------------------------
# Search Rate Limiting
# -----------------------------------------------------------------------------

search_rate_limit = 60              # Requests per minute (global)
max_concurrent_searches = 3         # Concurrent search requests

[research.per_provider_rate_limits]
tavily = 60
perplexity = 60
semantic_scholar = 100

# =============================================================================
# Test Runner Configuration
# =============================================================================
#
# Configure which test runner to use and customize runner settings.
# The foundry-setup command can auto-detect and configure this section.
#
# Supported runners (built-in defaults): pytest, go, npm, jest, make
# Custom runners can be defined in [test.runners.*] sections.

[test]
# Default runner to use when running tests
# Valid values: pytest, go, npm, jest, make, or any custom runner name
default_runner = "pytest"

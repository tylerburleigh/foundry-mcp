"""
Integration tests for authoring tools.

Tests:
- Tool registration and availability
- Hierarchy integrity after CRUD operations
- Response envelope compliance (response-v2)
- End-to-end authoring workflows
"""

import json
import pytest
from pathlib import Path
from foundry_mcp.server import create_server
from foundry_mcp.config import ServerConfig


# =============================================================================
# Test Fixtures
# =============================================================================


@pytest.fixture
def test_specs_dir(tmp_path):
    """Create a test specs directory with sample spec for authoring tests."""
    specs_dir = tmp_path / "specs"
    specs_dir.mkdir()
    (specs_dir / "active").mkdir()
    (specs_dir / "pending").mkdir()
    (specs_dir / "completed").mkdir()
    (specs_dir / "archived").mkdir()
    (specs_dir / "templates").mkdir()

    # Create a sample spec with complete hierarchy for authoring tests
    sample_spec = {
        "spec_id": "authoring-test-spec-001",
        "title": "Authoring Test Specification",
        "metadata": {
            "title": "Authoring Test Specification",
            "description": "A test spec for authoring tool integration testing",
            "created_at": "2025-01-25T00:00:00Z",
            "status": "in_progress",
            "version": "1.0.0",
        },
        "hierarchy": {
            "spec-root": {
                "type": "spec",
                "title": "Authoring Test Specification",
                "status": "in_progress",
                "children": ["phase-1", "phase-2"],
            },
            "phase-1": {
                "type": "phase",
                "title": "Setup Phase",
                "status": "completed",
                "parent": "spec-root",
                "children": ["task-1-1", "task-1-2"],
            },
            "phase-2": {
                "type": "phase",
                "title": "Implementation Phase",
                "status": "in_progress",
                "parent": "spec-root",
                "children": ["task-2-1"],
            },
            "task-1-1": {
                "type": "task",
                "title": "Initial setup",
                "status": "completed",
                "parent": "phase-1",
                "children": [],
            },
            "task-1-2": {
                "type": "task",
                "title": "Configuration",
                "status": "completed",
                "parent": "phase-1",
                "children": [],
            },
            "task-2-1": {
                "type": "task",
                "title": "Main implementation",
                "status": "in_progress",
                "parent": "phase-2",
                "children": ["subtask-2-1-1"],
            },
            "subtask-2-1-1": {
                "type": "subtask",
                "title": "Helper function",
                "status": "pending",
                "parent": "task-2-1",
                "children": [],
            },
        },
        "assumptions": [
            {
                "id": "assumption-1",
                "text": "API is available",
                "type": "constraint",
                "created_at": "2025-01-25T00:00:00Z",
            },
        ],
        "revision_history": [
            {
                "version": "1.0.0",
                "date": "2025-01-25",
                "changes": "Initial version",
                "author": "test",
            },
        ],
        "journal": [
            {
                "timestamp": "2025-01-25T00:00:00Z",
                "entry_type": "status_change",
                "title": "Spec created",
                "content": "Initial spec creation",
                "task_id": "spec-root",
            },
        ],
    }

    spec_file = specs_dir / "active" / "authoring-test-spec-001.json"
    with open(spec_file, "w") as f:
        json.dump(sample_spec, f, indent=2)

    return specs_dir


@pytest.fixture
def test_config(test_specs_dir):
    """Create a test server configuration."""
    return ServerConfig(
        server_name="foundry-mcp-authoring-test",
        server_version="0.1.0",
        specs_dir=test_specs_dir,
        log_level="WARNING",
    )


@pytest.fixture
def mcp_server(test_config):
    """Create a test MCP server instance."""
    return create_server(test_config)


# =============================================================================
# Authoring Tools Registration Tests
# =============================================================================


class TestAuthoringToolsRegistration:
    """Test that all authoring tools are properly registered."""

    def test_spec_create_registered(self, mcp_server):
        """Test that spec-create tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "spec-create" in tools
        assert callable(tools["spec-create"].fn)

    def test_spec_template_registered(self, mcp_server):
        """Test that spec-template tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "spec-template" in tools
        assert callable(tools["spec-template"].fn)

    def test_task_add_registered(self, mcp_server):
        """Test that task-add tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "task-add" in tools
        assert callable(tools["task-add"].fn)

    def test_task_remove_registered(self, mcp_server):
        """Test that task-remove tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "task-remove" in tools
        assert callable(tools["task-remove"].fn)

    def test_assumption_add_registered(self, mcp_server):
        """Test that assumption-add tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "assumption-add" in tools
        assert callable(tools["assumption-add"].fn)

    def test_assumption_list_registered(self, mcp_server):
        """Test that assumption-list tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "assumption-list" in tools
        assert callable(tools["assumption-list"].fn)

    def test_revision_add_registered(self, mcp_server):
        """Test that revision-add tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "revision-add" in tools
        assert callable(tools["revision-add"].fn)

    def test_spec_update_frontmatter_registered(self, mcp_server):
        """Test that spec-update-frontmatter tool is registered."""
        tools = mcp_server._tool_manager._tools
        assert "spec-update-frontmatter" in tools
        assert callable(tools["spec-update-frontmatter"].fn)

    def test_all_authoring_tools_count(self, mcp_server):
        """Test that all 8 authoring tools are registered."""
        tools = mcp_server._tool_manager._tools
        authoring_tools = [
            "spec-create",
            "spec-template",
            "task-add",
            "task-remove",
            "assumption-add",
            "assumption-list",
            "revision-add",
            "spec-update-frontmatter",
        ]
        registered = [t for t in authoring_tools if t in tools]
        assert len(registered) == 8, f"Expected 8 authoring tools, got {len(registered)}: {registered}"


# =============================================================================
# Response Envelope Compliance Tests
# =============================================================================


class TestResponseEnvelopeCompliance:
    """Test that authoring tools return response-v2 compliant envelopes."""

    def _validate_response_envelope(self, response):
        """Validate response follows response-v2 contract."""
        assert isinstance(response, dict), "Response must be a dict"
        assert "success" in response, "Response must have 'success' key"
        assert "data" in response, "Response must have 'data' key"
        assert "error" in response, "Response must have 'error' key"
        assert "meta" in response, "Response must have 'meta' key"
        assert isinstance(response["success"], bool), "success must be boolean"
        assert isinstance(response["data"], dict), "data must be dict"
        assert isinstance(response["meta"], dict), "meta must be dict"
        assert response["meta"].get("version") == "response-v2", "meta.version must be 'response-v2'"

        if response["success"]:
            assert response["error"] is None, "error must be null on success"
        else:
            assert response["error"] is not None, "error must be set on failure"
            assert isinstance(response["error"], str), "error must be string"

    def test_spec_template_list_envelope(self, mcp_server):
        """Test spec-template list action returns valid envelope."""
        tools = mcp_server._tool_manager._tools
        result = tools["spec-template"].fn(action="list")
        self._validate_response_envelope(result)

    def test_assumption_list_envelope(self, mcp_server):
        """Test assumption-list returns valid envelope."""
        tools = mcp_server._tool_manager._tools
        result = tools["assumption-list"].fn(spec_id="authoring-test-spec-001")
        self._validate_response_envelope(result)

    def test_validation_error_envelope(self, mcp_server):
        """Test validation errors return proper envelope with error details."""
        tools = mcp_server._tool_manager._tools
        # Missing required parameter
        result = tools["task-add"].fn(spec_id="", parent="phase-1", title="Test")
        self._validate_response_envelope(result)
        assert result["success"] is False
        assert "error_code" in result["data"]
        assert "error_type" in result["data"]
        assert "remediation" in result["data"]

    def test_spec_create_invalid_template_envelope(self, mcp_server):
        """Test spec-create with invalid template returns proper error envelope."""
        tools = mcp_server._tool_manager._tools
        result = tools["spec-create"].fn(name="test", template="invalid_template")
        self._validate_response_envelope(result)
        assert result["success"] is False
        assert "VALIDATION_ERROR" in str(result["data"].get("error_code", ""))


# =============================================================================
# Hierarchy Integrity Tests
# =============================================================================


class TestHierarchyIntegrity:
    """Test that authoring operations maintain hierarchy integrity."""

    def test_task_add_validates_parent_exists(self, mcp_server):
        """Test that task-add validates parent node exists."""
        tools = mcp_server._tool_manager._tools

        # Try to add task with non-existent parent
        result = tools["task-add"].fn(
            spec_id="authoring-test-spec-001",
            parent="nonexistent-phase",
            title="Orphan task",
        )

        assert result["success"] is False
        assert "not found" in result["error"].lower() or "NOT_FOUND" in str(result["data"].get("error_code", ""))

    def test_task_remove_validates_task_exists(self, mcp_server):
        """Test that task-remove validates task exists."""
        tools = mcp_server._tool_manager._tools

        # Try to remove non-existent task
        result = tools["task-remove"].fn(
            spec_id="authoring-test-spec-001",
            task_id="nonexistent-task",
        )

        assert result["success"] is False
        assert "not found" in result["error"].lower() or "NOT_FOUND" in str(result["data"].get("error_code", ""))

    def test_task_remove_cascade_warning(self, mcp_server):
        """Test that removing task with children requires cascade flag."""
        tools = mcp_server._tool_manager._tools

        # Try to remove task with children without cascade
        result = tools["task-remove"].fn(
            spec_id="authoring-test-spec-001",
            task_id="task-2-1",  # Has subtask-2-1-1 as child
            cascade=False,
        )

        # Should either succeed with warning or fail
        # CLI might not find spec (different path) or might check cascade
        if result["success"] is False:
            # Accept either cascade warning or not found (CLI path mismatch)
            error_lower = result["error"].lower()
            assert (
                "children" in error_lower or
                "cascade" in error_lower or
                "not found" in error_lower
            )

    def test_spec_not_found_error_handling(self, mcp_server):
        """Test proper error handling when spec doesn't exist."""
        tools = mcp_server._tool_manager._tools

        # Try operations on non-existent spec
        result = tools["task-add"].fn(
            spec_id="nonexistent-spec-999",
            parent="phase-1",
            title="Test task",
        )

        assert result["success"] is False
        error_msg = result["error"].lower()
        error_code = str(result["data"].get("error_code", ""))
        assert "not found" in error_msg or "SPEC_NOT_FOUND" in error_code or "NOT_FOUND" in error_code


# =============================================================================
# Metadata Operations Tests
# =============================================================================


class TestMetadataOperations:
    """Test metadata-related authoring operations."""

    def test_assumption_list_returns_existing_assumptions(self, mcp_server):
        """Test that assumption-list returns existing assumptions."""
        tools = mcp_server._tool_manager._tools

        result = tools["assumption-list"].fn(spec_id="authoring-test-spec-001")

        # May succeed or fail depending on CLI availability
        if result["success"]:
            assert "assumptions" in result["data"]
            assert isinstance(result["data"]["assumptions"], list)

    def test_assumption_add_validation(self, mcp_server):
        """Test that assumption-add validates inputs."""
        tools = mcp_server._tool_manager._tools

        # Missing text
        result = tools["assumption-add"].fn(
            spec_id="authoring-test-spec-001",
            text="",
        )

        assert result["success"] is False
        assert "text" in result["error"].lower()

    def test_assumption_type_validation(self, mcp_server):
        """Test that assumption type is validated."""
        tools = mcp_server._tool_manager._tools

        result = tools["assumption-add"].fn(
            spec_id="authoring-test-spec-001",
            text="Test assumption",
            assumption_type="invalid_type",
        )

        assert result["success"] is False
        assert "invalid_type" in result["error"].lower()

    def test_revision_add_validation(self, mcp_server):
        """Test that revision-add validates inputs."""
        tools = mcp_server._tool_manager._tools

        # Missing version
        result = tools["revision-add"].fn(
            spec_id="authoring-test-spec-001",
            version="",
            changes="Test changes",
        )

        assert result["success"] is False
        assert "version" in result["error"].lower()

    def test_frontmatter_update_validation(self, mcp_server):
        """Test that frontmatter update validates inputs."""
        tools = mcp_server._tool_manager._tools

        # Missing key
        result = tools["spec-update-frontmatter"].fn(
            spec_id="authoring-test-spec-001",
            key="",
            value="test",
        )

        assert result["success"] is False
        assert "key" in result["error"].lower()


# =============================================================================
# Template Operations Tests
# =============================================================================


class TestTemplateOperations:
    """Test spec-template tool operations."""

    def test_template_list_action(self, mcp_server):
        """Test template list action."""
        tools = mcp_server._tool_manager._tools

        result = tools["spec-template"].fn(action="list")

        # CLI may not be available; verify we get a valid response envelope
        assert "success" in result
        assert "data" in result
        # If successful, should have action and templates data
        if result["success"]:
            assert result["data"]["action"] == "list"
            assert "templates" in result["data"] or "total_count" in result["data"]

    def test_template_show_requires_name(self, mcp_server):
        """Test template show action requires template_name."""
        tools = mcp_server._tool_manager._tools

        result = tools["spec-template"].fn(action="show")

        assert result["success"] is False
        assert "template_name" in result["error"].lower()

    def test_template_apply_requires_name(self, mcp_server):
        """Test template apply action requires template_name."""
        tools = mcp_server._tool_manager._tools

        result = tools["spec-template"].fn(action="apply")

        assert result["success"] is False
        assert "template_name" in result["error"].lower()

    def test_template_invalid_action(self, mcp_server):
        """Test template with invalid action."""
        tools = mcp_server._tool_manager._tools

        result = tools["spec-template"].fn(action="invalid_action")

        assert result["success"] is False
        assert "invalid_action" in result["error"].lower()


# =============================================================================
# Spec Creation Tests
# =============================================================================


class TestSpecCreation:
    """Test spec-create tool operations."""

    def test_spec_create_validates_template(self, mcp_server):
        """Test spec-create validates template parameter."""
        tools = mcp_server._tool_manager._tools

        # Valid templates are: simple, medium, complex, security
        result = tools["spec-create"].fn(
            name="test-spec",
            template="invalid_template",
        )

        assert result["success"] is False
        assert "VALIDATION_ERROR" in str(result["data"].get("error_code", ""))

    def test_spec_create_validates_category(self, mcp_server):
        """Test spec-create validates category parameter."""
        tools = mcp_server._tool_manager._tools

        # Valid categories are: investigation, implementation, refactoring, decision, research
        result = tools["spec-create"].fn(
            name="test-spec",
            category="invalid_category",
        )

        assert result["success"] is False
        assert "invalid_category" in result["error"].lower()

    def test_spec_create_valid_templates_accepted(self, mcp_server):
        """Test spec-create accepts all valid templates."""
        tools = mcp_server._tool_manager._tools

        valid_templates = ["simple", "medium", "complex", "security"]
        for template in valid_templates:
            result = tools["spec-create"].fn(
                name=f"test-{template}",
                template=template,
            )
            # May fail due to CLI not being available, but should not fail validation
            if result["success"] is False:
                assert "VALIDATION_ERROR" not in str(result["data"].get("error_code", "")), (
                    f"Template '{template}' should be valid"
                )

    def test_spec_create_valid_categories_accepted(self, mcp_server):
        """Test spec-create accepts all valid categories."""
        tools = mcp_server._tool_manager._tools

        valid_categories = ["investigation", "implementation", "refactoring", "decision", "research"]
        for category in valid_categories:
            result = tools["spec-create"].fn(
                name=f"test-{category}",
                category=category,
            )
            # May fail due to CLI not being available, but should not fail validation
            if result["success"] is False:
                assert category.lower() not in result["error"].lower(), (
                    f"Category '{category}' should be valid"
                )


# =============================================================================
# Task Operations Tests
# =============================================================================


class TestTaskOperations:
    """Test task-add and task-remove operations."""

    def test_task_add_validates_spec_id(self, mcp_server):
        """Test task-add validates spec_id."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-add"].fn(
            spec_id="",
            parent="phase-1",
            title="Test task",
        )

        assert result["success"] is False
        assert "spec_id" in result["error"].lower()

    def test_task_add_validates_parent(self, mcp_server):
        """Test task-add validates parent."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-add"].fn(
            spec_id="test-spec",
            parent="",
            title="Test task",
        )

        assert result["success"] is False
        assert "parent" in result["error"].lower()

    def test_task_add_validates_title(self, mcp_server):
        """Test task-add validates title."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-add"].fn(
            spec_id="test-spec",
            parent="phase-1",
            title="",
        )

        assert result["success"] is False
        assert "title" in result["error"].lower()

    def test_task_add_validates_task_type(self, mcp_server):
        """Test task-add validates task_type."""
        tools = mcp_server._tool_manager._tools

        # Valid types are: task, subtask, verify
        result = tools["task-add"].fn(
            spec_id="test-spec",
            parent="phase-1",
            title="Test",
            task_type="invalid_type",
        )

        assert result["success"] is False
        assert "invalid_type" in result["error"].lower()

    def test_task_add_accepts_valid_types(self, mcp_server):
        """Test task-add accepts all valid task types."""
        tools = mcp_server._tool_manager._tools

        valid_types = ["task", "subtask", "verify"]
        for task_type in valid_types:
            result = tools["task-add"].fn(
                spec_id="authoring-test-spec-001",
                parent="phase-1",
                title=f"Test {task_type}",
                task_type=task_type,
            )
            # May fail due to CLI, but should not fail type validation
            if result["success"] is False:
                assert task_type.lower() not in result["error"].lower(), (
                    f"Task type '{task_type}' should be valid"
                )

    def test_task_remove_validates_spec_id(self, mcp_server):
        """Test task-remove validates spec_id."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-remove"].fn(
            spec_id="",
            task_id="task-1-1",
        )

        assert result["success"] is False
        assert "spec_id" in result["error"].lower()

    def test_task_remove_validates_task_id(self, mcp_server):
        """Test task-remove validates task_id."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-remove"].fn(
            spec_id="test-spec",
            task_id="",
        )

        assert result["success"] is False
        assert "task_id" in result["error"].lower()

    def test_task_add_dry_run_option(self, mcp_server):
        """Test task-add supports dry_run option."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-add"].fn(
            spec_id="authoring-test-spec-001",
            parent="phase-1",
            title="Dry run task",
            dry_run=True,
        )

        # Should either succeed with dry_run flag set or fail gracefully
        if result["success"]:
            assert result["data"].get("dry_run") is True

    def test_task_remove_dry_run_option(self, mcp_server):
        """Test task-remove supports dry_run option."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-remove"].fn(
            spec_id="authoring-test-spec-001",
            task_id="task-1-1",
            dry_run=True,
        )

        # Should either succeed with dry_run flag set or fail gracefully
        if result["success"]:
            assert result["data"].get("dry_run") is True


# =============================================================================
# End-to-End Workflow Tests
# =============================================================================


class TestAuthoringWorkflows:
    """Test end-to-end authoring workflows."""

    def test_list_then_add_assumption_workflow(self, mcp_server):
        """Test workflow: list assumptions, then add new one."""
        tools = mcp_server._tool_manager._tools

        # Step 1: List existing assumptions
        list_result = tools["assumption-list"].fn(spec_id="authoring-test-spec-001")

        # Step 2: Add a new assumption
        add_result = tools["assumption-add"].fn(
            spec_id="authoring-test-spec-001",
            text="New integration test assumption",
            assumption_type="constraint",
        )

        # Both operations should return valid envelopes
        assert "success" in list_result
        assert "success" in add_result

    def test_template_list_then_show_workflow(self, mcp_server):
        """Test workflow: list templates, then show one."""
        tools = mcp_server._tool_manager._tools

        # Step 1: List templates
        list_result = tools["spec-template"].fn(action="list")
        # CLI may not be available; verify we get a valid envelope
        assert "success" in list_result
        assert "data" in list_result

        # Step 2: If templates exist and successful, try to show one
        if list_result["success"] and list_result["data"].get("templates"):
            template_name = list_result["data"]["templates"][0].get("name", "simple")
            show_result = tools["spec-template"].fn(
                action="show",
                template_name=template_name,
            )
            assert "success" in show_result

    def test_validation_before_creation_workflow(self, mcp_server):
        """Test workflow: validate inputs before creating resources."""
        tools = mcp_server._tool_manager._tools

        # Step 1: Try invalid spec creation (should fail validation)
        invalid_result = tools["spec-create"].fn(
            name="test",
            template="not_a_real_template",
        )
        assert invalid_result["success"] is False
        assert invalid_result["data"].get("error_code") == "VALIDATION_ERROR"

        # Step 2: Try valid parameters (may fail due to CLI, but should pass validation)
        valid_result = tools["spec-create"].fn(
            name="test",
            template="simple",
        )
        # Should not fail validation
        if valid_result["success"] is False:
            assert valid_result["data"].get("error_code") != "VALIDATION_ERROR"


# =============================================================================
# Error Handling Tests
# =============================================================================


class TestErrorHandling:
    """Test error handling in authoring tools."""

    def test_circuit_breaker_error_response(self, mcp_server):
        """Test that circuit breaker errors are properly handled."""
        # This test verifies the error response format when circuit breaker is open
        # Since we can't easily trigger the circuit breaker in integration tests,
        # we test that the error codes are defined correctly
        tools = mcp_server._tool_manager._tools

        # Make a request that will likely fail
        result = tools["spec-create"].fn(name="test")

        # Should return valid envelope regardless of success
        assert "success" in result
        assert "data" in result
        assert "error" in result
        assert "meta" in result

    def test_timeout_error_includes_remediation(self, mcp_server):
        """Test that timeout errors include remediation guidance."""
        tools = mcp_server._tool_manager._tools

        # Trigger a request that might timeout
        result = tools["task-add"].fn(
            spec_id="authoring-test-spec-001",
            parent="phase-1",
            title="Test task",
        )

        # If it failed with timeout, check remediation
        if result["success"] is False and "TIMEOUT" in str(result["data"].get("error_code", "")):
            assert "remediation" in result["data"]

    def test_not_found_errors_include_remediation(self, mcp_server):
        """Test that not found errors include remediation guidance."""
        tools = mcp_server._tool_manager._tools

        result = tools["task-add"].fn(
            spec_id="nonexistent-spec-999",
            parent="phase-1",
            title="Test task",
        )

        if result["success"] is False:
            # Should include remediation for not found errors
            if "NOT_FOUND" in str(result["data"].get("error_code", "")) or "not found" in result["error"].lower():
                assert "remediation" in result["data"]
